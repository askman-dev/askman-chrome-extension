[chatglm]
sdk = "openai"
api_key = "" 
base_url = "https://extapi.askman.dev/v1/"
send_api_key = false
cloudflare_gateway_url = ""
models = [
  { name = "free", max_tokens = 32768 } # THUDM/glm-4-9b-chat via siliconflow
]

# [openai]
# sdk = "openai"  # Protocol used to send requests
# api_key = "sk-xxx" # Your OpenAI API key
# base_url = "https://api.openai.com/v1" # Concatenation logic is {base_url}/chat/completions
# send_api_key = true # Whether to include the API Key in headers
# cloudflare_gateway_url = "" # Use Cloudflare Gateway proxy
# models = [
#   { name = "gpt-3.5-turbo", max_tokens = 4096 },
#   { name = "gpt-4", max_tokens = 8192 }
# ]

# The following is not yet supported
# [azure]
# api_key = ""
# endpoint = ""
# models = [
#   { name = "gpt-3.5-turbo", max_tokens = 4096 },
#   { name = "gpt-4", max_tokens = 8192 }
# ]

# [anthropic]
# api_key = ""
# models = [
#   { name = "claude-2", max_tokens = 100000 },
#   { name = "claude-instant-1", max_tokens = 100000 }
# ]

# [google]
# api_key = ""
# models = [
#   { name = "gemini-pro", max_tokens = 30720 },
#   { name = "text-bison", max_tokens = 8192 }
# ]

# [groq]
# api_key = ""
# models = [
#   { name = "llama2-70b-4096", max_tokens = 4096 },
#   { name = "mixtral-8x7b-32768", max_tokens = 32768 }
# ]

# [ollama]
# api_base = "http://localhost:11434"
# models = [
#   { name = "llama2", max_tokens = 4096 },
#   { name = "mistral", max_tokens = 8192 }
# ]

# [openrouter]
# api_key = ""
# models = []
